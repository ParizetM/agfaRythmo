#!/usr/bin/env python3
"""
Script d'extraction automatique de dialogues avec Whisper + diarization
Optimis√© pour serveurs avec 2GB RAM

Usage:
    python extract_dialogues.py <video_path> <output_json> [--model tiny] [--language auto] [--max-speakers 10]

Output JSON format:
    {
        "dialogues": [
            {
                "start": 1.5,
                "end": 4.2,
                "text": "Hello, how are you?",
                "speaker": "SPEAKER_00",
                "confidence": 0.95,
                "language": "en"
            },
            ...
        ],
        "speakers": ["SPEAKER_00", "SPEAKER_01", ...],
        "metadata": {
            "duration": 120.5,
            "total_dialogues": 42,
            "model": "tiny",
            "language": "auto"
        }
    }
"""

import argparse
import json
import os
import sys
import tempfile
import subprocess
from pathlib import Path
from typing import List, Dict, Optional
import gc

# V√©rifier les d√©pendances
try:
    import whisper
    import torch
except ImportError:
    print("‚ùå Erreur: whisper ou torch non install√©", file=sys.stderr)
    print("Installation: pip install openai-whisper torch", file=sys.stderr)
    sys.exit(1)



class DialogueExtractor:
    """Extracteur de dialogues optimis√© pour faible RAM"""

    # Mod√®les Whisper par taille (RAM requise approximative)
    MODELS = {
        'tiny': 1,      # ~1GB RAM
        'base': 1,      # ~1GB RAM
        'small': 2,     # ~2GB RAM
        'medium': 5,    # ~5GB RAM (non recommand√© pour 2GB serveur)
        'large': 10,    # ~10GB RAM (impossible sur 2GB serveur)
    }

    # Langues support√©es par Whisper
    SUPPORTED_LANGUAGES = [
        'auto', 'en', 'fr', 'es', 'de', 'it', 'pt', 'nl', 'ru', 'zh', 'ja', 'ko'
    ]

    def __init__(self, model_name: str = 'tiny', language: str = 'auto', max_speakers: int = 10):
        """
        Initialize dialogue extractor

        Args:
            model_name: Whisper model (tiny/base/small)
            language: Language code or 'auto' for detection
            max_speakers: Maximum number of speakers to detect
        """
        if model_name not in self.MODELS:
            raise ValueError(f"Model invalide. Choisir parmi: {list(self.MODELS.keys())}")

        if language not in self.SUPPORTED_LANGUAGES:
            raise ValueError(f"Langue non support√©e: {language}")

        self.model_name = model_name
        self.language = None if language == 'auto' else language
        self.max_speakers = max_speakers
        self.model = None
        self.diarization_pipeline = None

        print(f"üîß Configuration: model={model_name}, language={language}, max_speakers={max_speakers}")

    def _load_whisper_model(self):
        """Charger le mod√®le Whisper (lazy loading)"""
        if self.model is None:
            print(f"üì• Chargement du mod√®le Whisper '{self.model_name}'...")
            # Force CPU pour √©conomiser RAM (GPU optionnel si disponible)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = whisper.load_model(self.model_name, device=device)
            print(f"‚úÖ Mod√®le charg√© sur {device}")

    def _unload_whisper_model(self):
        """D√©charger le mod√®le Whisper pour lib√©rer RAM"""
        if self.model is not None:
            del self.model
            self.model = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("üóëÔ∏è  Mod√®le Whisper d√©charg√© (RAM lib√©r√©e)")

    def extract_audio(self, video_path: str) -> str:
        """
        Extraire l'audio de la vid√©o avec FFmpeg

        Args:
            video_path: Chemin vers la vid√©o

        Returns:
            Chemin vers le fichier audio temporaire (WAV 16kHz mono)
        """
        print(f"üéµ Extraction audio depuis: {video_path}")

        if not os.path.exists(video_path):
            raise FileNotFoundError(f"Vid√©o introuvable: {video_path}")

        # Cr√©er fichier audio temporaire
        temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        audio_path = temp_audio.name
        temp_audio.close()

        # Extraire audio avec FFmpeg (16kHz mono pour Whisper)
        cmd = [
            'ffmpeg', '-i', video_path,
            '-vn',  # Pas de vid√©o
            '-acodec', 'pcm_s16le',  # WAV 16-bit
            '-ar', '16000',  # 16kHz
            '-ac', '1',  # Mono
            '-y',  # Overwrite
            audio_path
        ]

        try:
            subprocess.run(cmd, check=True, capture_output=True, text=True)
            print(f"‚úÖ Audio extrait: {audio_path}")
            return audio_path
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Erreur FFmpeg: {e.stderr}")

    def transcribe_audio(self, audio_path: str) -> Dict:
        """
        Transcrire l'audio avec Whisper

        Args:
            audio_path: Chemin vers le fichier audio

        Returns:
            Dictionnaire avec segments transcrits
        """
        self._load_whisper_model()

        print(f"üé§ Transcription en cours...")

        # Options Whisper optimis√©es pour m√©moire
        options = {
            'language': self.language,
            'task': 'transcribe',
            'verbose': False,
            'word_timestamps': True,  # Timestamps pr√©cis par mot
        }

        # Transcription
        result = self.model.transcribe(audio_path, **options)

        print(f"‚úÖ Transcription termin√©e: {len(result['segments'])} segments")

        return result

    def apply_diarization(self, audio_path: str, transcription: Dict) -> List[Dict]:
        """
        Appliquer la diarization (s√©paration locuteurs) avec clustering MFCC ultra-light
        Optimis√© pour serveurs 2GB RAM - pas de deep learning

        Args:
            audio_path: Chemin vers l'audio
            transcription: R√©sultat Whisper

        Returns:
            Liste de dialogues avec speakers assign√©s
        """
        # V√©rifier si diarization activ√©e
        diarization_enabled = os.getenv('AI_DIARIZATION_ENABLED', 'false').lower() == 'true'
        
        if not diarization_enabled:
            print("", file=sys.stderr)
            print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó", file=sys.stderr)
            print("‚ïë  ‚ö†Ô∏è  DIARIZATION D√âSACTIV√âE                                   ‚ïë", file=sys.stderr)
            print("‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£", file=sys.stderr)
            print("‚ïë  AI_DIARIZATION_ENABLED=false dans le fichier .env            ‚ïë", file=sys.stderr)
            print("‚ïë                                                                ‚ïë", file=sys.stderr)
            print("‚ïë  R√©sultat : UN SEUL locuteur (SPEAKER_00)                      ‚ïë", file=sys.stderr)
            print("‚ïë                                                                ‚ïë", file=sys.stderr)
            print("‚ïë  Pour activer la d√©tection multi-locuteurs :                  ‚ïë", file=sys.stderr)
            print("‚ïë  Mettre AI_DIARIZATION_ENABLED=true dans .env                 ‚ïë", file=sys.stderr)
            print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù", file=sys.stderr)
            print("", file=sys.stderr)
            return self._assign_single_speaker(transcription)

        # Utiliser le script de diarization ultra-light (clustering MFCC)
        try:
            # Sauvegarder temporairement la transcription
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as tf:
                json.dump(transcription, tf, ensure_ascii=False)
                transcription_temp = tf.name
            
            # Fichier output temporaire
            output_temp = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False).name
            
            # Appeler le script de diarization
            script_path = Path(__file__).parent / 'simple_diarization.py'
            cmd = [
                sys.executable,
                str(script_path),
                audio_path,
                transcription_temp,
                output_temp,
                '--max-speakers', str(self.max_speakers)
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            # Afficher stderr (progression)
            if result.stderr:
                print(result.stderr, file=sys.stderr, end='')
            
            # Charger r√©sultat
            with open(output_temp, 'r', encoding='utf-8') as f:
                diarization_result = json.load(f)
            
            # Nettoyer fichiers temporaires
            os.unlink(transcription_temp)
            os.unlink(output_temp)
            
            return diarization_result['dialogues']
            
        except subprocess.CalledProcessError as e:
            print(f"‚ùå ERREUR Diarization: {e.stderr}", file=sys.stderr)
            print("‚ö†Ô∏è  Fallback: Attribution de tous les dialogues √† SPEAKER_00", file=sys.stderr)
            return self._assign_single_speaker(transcription)
        except Exception as e:
            print(f"‚ùå ERREUR Diarization: {str(e)}", file=sys.stderr)
            print("‚ö†Ô∏è  Fallback: Attribution de tous les dialogues √† SPEAKER_00", file=sys.stderr)
            return self._assign_single_speaker(transcription)

    def _assign_single_speaker(self, transcription: Dict) -> List[Dict]:
        """
        Assigner tous les segments √† un seul speaker (fallback)

        Args:
            transcription: R√©sultat Whisper

        Returns:
            Liste de dialogues avec un seul speaker
        """
        dialogues = []

        for segment in transcription['segments']:
            dialogues.append({
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'].strip(),
                'speaker': 'SPEAKER_00',
                'confidence': segment.get('confidence', segment.get('no_speech_prob', 0.0)),
                'language': transcription.get('language', 'unknown')
            })

        return dialogues

    def process_video(self, video_path: str, output_json: str) -> Dict:
        """
        Pipeline complet d'extraction de dialogues

        Args:
            video_path: Chemin vers la vid√©o
            output_json: Chemin de sortie JSON

        Returns:
            Donn√©es extraites
        """
        audio_path = None

        try:
            # √âtape 1: Extraction audio (0-20%)
            audio_path = self.extract_audio(video_path)

            # √âtape 2: Transcription Whisper (20-70%)
            transcription = self.transcribe_audio(audio_path)

            # Lib√©rer m√©moire apr√®s transcription
            self._unload_whisper_model()

            # √âtape 3: Diarization (70-90%)
            dialogues = self.apply_diarization(audio_path, transcription)

            # √âtape 4: Formater r√©sultat (90-100%)
            speakers = list(set(d['speaker'] for d in dialogues))
            speakers.sort()

            result = {
                'dialogues': dialogues,
                'speakers': speakers,
                'metadata': {
                    'duration': transcription.get('duration', 0),
                    'total_dialogues': len(dialogues),
                    'model': self.model_name,
                    'language': transcription.get('language', 'unknown'),
                    'detected_speakers': len(speakers)
                }
            }

            # Sauvegarder JSON
            with open(output_json, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"‚úÖ R√©sultat sauvegard√©: {output_json}")
            print(f"üìä Statistiques:")
            print(f"   - Dialogues: {len(dialogues)}")
            print(f"   - Locuteurs: {len(speakers)}")
            print(f"   - Dur√©e: {result['metadata']['duration']:.1f}s")
            print(f"   - Langue d√©tect√©e: {result['metadata']['language']}")

            return result

        finally:
            # Nettoyage fichier audio temporaire
            if audio_path and os.path.exists(audio_path):
                os.unlink(audio_path)
                print(f"üóëÔ∏è  Fichier audio temporaire supprim√©")

            # Lib√©rer m√©moire
            self._unload_whisper_model()
            gc.collect()


def main():
    parser = argparse.ArgumentParser(
        description="Extraction automatique de dialogues avec Whisper + diarization"
    )
    parser.add_argument('video_path', help="Chemin vers la vid√©o")
    parser.add_argument('output_json', help="Chemin de sortie JSON")
    parser.add_argument('--model', default='tiny', choices=list(DialogueExtractor.MODELS.keys()),
                        help="Mod√®le Whisper (d√©faut: tiny)")
    parser.add_argument('--language', default='auto',
                        help="Langue source (auto/en/fr/zh/ja/..., d√©faut: auto)")
    parser.add_argument('--max-speakers', type=int, default=10,
                        help="Nombre max de locuteurs (d√©faut: 10)")

    args = parser.parse_args()

    try:
        extractor = DialogueExtractor(
            model_name=args.model,
            language=args.language,
            max_speakers=args.max_speakers
        )

        result = extractor.process_video(args.video_path, args.output_json)

        print("\n‚úÖ Extraction termin√©e avec succ√®s!")
        sys.exit(0)

    except Exception as e:
        print(f"\n‚ùå Erreur: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
