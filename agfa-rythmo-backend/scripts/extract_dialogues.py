#!/usr/bin/env python3
"""
Script d'extraction automatique de dialogues avec Whisper + diarization
OptimisÃ© pour serveurs avec 2GB RAM

Usage:
    python extract_dialogues.py <video_path> <output_json> [--model tiny] [--language auto] [--max-speakers 10]

Output JSON format:
    {
        "dialogues": [
            {
                "start": 1.5,
                "end": 4.2,
                "text": "Hello, how are you?",
                "speaker": "SPEAKER_00",
                "confidence": 0.95,
                "language": "en"
            },
            ...
        ],
        "speakers": ["SPEAKER_00", "SPEAKER_01", ...],
        "metadata": {
            "duration": 120.5,
            "total_dialogues": 42,
            "model": "tiny",
            "language": "auto"
        }
    }
"""

import argparse
import json
import os
import sys
import tempfile
import subprocess
from pathlib import Path
from typing import List, Dict, Optional
import gc

# VÃ©rifier les dÃ©pendances
try:
    import whisper
    import torch
    import soundfile as sf
    import numpy as np
except ImportError:
    print("âŒ Erreur: whisper ou torch non installÃ©", file=sys.stderr)
    print("Installation: pip install openai-whisper torch soundfile numpy", file=sys.stderr)
    sys.exit(1)

# Diarization est optionnelle
DIARIZATION_AVAILABLE = False
try:
    from pyannote.audio import Pipeline
    DIARIZATION_AVAILABLE = True
except ImportError:
    print("âš ï¸  Warning: pyannote.audio non installÃ©, diarization dÃ©sactivÃ©e", file=sys.stderr)
    print("Installation: pip install pyannote.audio", file=sys.stderr)


class DialogueExtractor:
    """Extracteur de dialogues optimisÃ© pour faible RAM"""

    # ModÃ¨les Whisper par taille (RAM requise approximative)
    MODELS = {
        'tiny': 1,      # ~1GB RAM
        'base': 1,      # ~1GB RAM
        'small': 2,     # ~2GB RAM
        'medium': 5,    # ~5GB RAM (non recommandÃ© pour 2GB serveur)
        'large': 10,    # ~10GB RAM (impossible sur 2GB serveur)
    }

    # Langues supportÃ©es par Whisper
    SUPPORTED_LANGUAGES = [
        'auto', 'en', 'fr', 'es', 'de', 'it', 'pt', 'nl', 'ru', 'zh', 'ja', 'ko'
    ]

    def __init__(self, model_name: str = 'tiny', language: str = 'auto', max_speakers: int = 10):
        """
        Initialize dialogue extractor

        Args:
            model_name: Whisper model (tiny/base/small)
            language: Language code or 'auto' for detection
            max_speakers: Maximum number of speakers to detect
        """
        if model_name not in self.MODELS:
            raise ValueError(f"Model invalide. Choisir parmi: {list(self.MODELS.keys())}")

        if language not in self.SUPPORTED_LANGUAGES:
            raise ValueError(f"Langue non supportÃ©e: {language}")

        self.model_name = model_name
        self.language = None if language == 'auto' else language
        self.max_speakers = max_speakers
        self.model = None
        self.diarization_pipeline = None

        print(f"ğŸ”§ Configuration: model={model_name}, language={language}, max_speakers={max_speakers}")

    def _load_whisper_model(self):
        """Charger le modÃ¨le Whisper (lazy loading)"""
        if self.model is None:
            print(f"ğŸ“¥ Chargement du modÃ¨le Whisper '{self.model_name}'...")
            # Force CPU pour Ã©conomiser RAM (GPU optionnel si disponible)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = whisper.load_model(self.model_name, device=device)
            print(f"âœ… ModÃ¨le chargÃ© sur {device}")

    def _unload_whisper_model(self):
        """DÃ©charger le modÃ¨le Whisper pour libÃ©rer RAM"""
        if self.model is not None:
            del self.model
            self.model = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print("ğŸ—‘ï¸  ModÃ¨le Whisper dÃ©chargÃ© (RAM libÃ©rÃ©e)")

    def extract_audio(self, video_path: str) -> str:
        """
        Extraire l'audio de la vidÃ©o avec FFmpeg

        Args:
            video_path: Chemin vers la vidÃ©o

        Returns:
            Chemin vers le fichier audio temporaire (WAV 16kHz mono)
        """
        print(f"ğŸµ Extraction audio depuis: {video_path}")

        if not os.path.exists(video_path):
            raise FileNotFoundError(f"VidÃ©o introuvable: {video_path}")

        # CrÃ©er fichier audio temporaire
        temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
        audio_path = temp_audio.name
        temp_audio.close()

        # Extraire audio avec FFmpeg (16kHz mono pour Whisper)
        cmd = [
            'ffmpeg', '-i', video_path,
            '-vn',  # Pas de vidÃ©o
            '-acodec', 'pcm_s16le',  # WAV 16-bit
            '-ar', '16000',  # 16kHz
            '-ac', '1',  # Mono
            '-y',  # Overwrite
            audio_path
        ]

        try:
            subprocess.run(cmd, check=True, capture_output=True, text=True)
            print(f"âœ… Audio extrait: {audio_path}")
            return audio_path
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Erreur FFmpeg: {e.stderr}")

    def transcribe_audio(self, audio_path: str) -> Dict:
        """
        Transcrire l'audio avec Whisper

        Args:
            audio_path: Chemin vers le fichier audio

        Returns:
            Dictionnaire avec segments transcrits
        """
        self._load_whisper_model()

        print(f"ğŸ¤ Transcription en cours...")

        # Options Whisper optimisÃ©es pour mÃ©moire
        options = {
            'language': self.language,
            'task': 'transcribe',
            'verbose': False,
            'word_timestamps': True,  # Timestamps prÃ©cis par mot
        }

        # Transcription
        result = self.model.transcribe(audio_path, **options)

        print(f"âœ… Transcription terminÃ©e: {len(result['segments'])} segments")

        return result

    def apply_diarization(self, audio_path: str, transcription: Dict) -> List[Dict]:
        """
        Appliquer la diarization (sÃ©paration locuteurs)

        Args:
            audio_path: Chemin vers l'audio
            transcription: RÃ©sultat Whisper

        Returns:
            Liste de dialogues avec speakers assignÃ©s
        """
        if not DIARIZATION_AVAILABLE:
            print("", file=sys.stderr)
            print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", file=sys.stderr)
            print("â•‘  âš ï¸  DIARIZATION DÃ‰SACTIVÃ‰E                                   â•‘", file=sys.stderr)
            print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£", file=sys.stderr)
            print("â•‘  AI_DIARIZATION_ENABLED=false dans le fichier .env            â•‘", file=sys.stderr)
            print("â•‘                                                                â•‘", file=sys.stderr)
            print("â•‘  RÃ©sultat : UN SEUL locuteur (SPEAKER_00)                      â•‘", file=sys.stderr)
            print("â•‘                                                                â•‘", file=sys.stderr)
            print("â•‘  Pour activer la dÃ©tection multi-locuteurs :                  â•‘", file=sys.stderr)
            print("â•‘  1. Mettre AI_DIARIZATION_ENABLED=true dans .env              â•‘", file=sys.stderr)
            print("â•‘  2. Configurer HF_TOKEN (voir .env.example)                   â•‘", file=sys.stderr)
            print("â•‘  3. Installer: pip install pyannote.audio                     â•‘", file=sys.stderr)
            print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
            print("", file=sys.stderr)
            return self._assign_single_speaker(transcription)

        print("", file=sys.stderr)
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", file=sys.stderr)
        print("â•‘  ğŸ‘¥ DIARIZATION ACTIVÃ‰E - DÃ©tection des locuteurs            â•‘", file=sys.stderr)
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
        print("", file=sys.stderr)

        try:
            # Charger le modÃ¨le de diarization si pas dÃ©jÃ  fait
            if self.diarization_pipeline is None:
                from pyannote.audio import Pipeline
                print("ğŸ“¥ Chargement du modÃ¨le de diarization...", file=sys.stderr)

                # NÃ©cessite un token HuggingFace (gratuit)
                # Export HF_TOKEN="your_token" dans .env ou environnement
                hf_token = os.getenv('HF_TOKEN') or os.getenv('HUGGINGFACE_TOKEN')

                # Nettoyer le token (enlever guillemets si prÃ©sents)
                if hf_token:
                    hf_token = hf_token.strip().strip("'\"")

                if not hf_token:
                    print("âš ï¸  Warning: HF_TOKEN non trouvÃ©, diarization peut Ã©chouer", file=sys.stderr)
                    print("âš ï¸  Obtenez un token gratuit sur: https://huggingface.co/settings/tokens", file=sys.stderr)

                self.diarization_pipeline = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    token=hf_token  # ChangÃ© de use_auth_token Ã  token (nouvelle API pyannote)
                )
                print("âœ… ModÃ¨le de diarization chargÃ©", file=sys.stderr)

            # Appliquer la diarization
            print(f"ğŸ” Analyse des locuteurs (max {self.max_speakers})...", file=sys.stderr)
            
            # Charger l'audio avec soundfile (Ã©vite le problÃ¨me torchcodec)
            waveform, sample_rate = sf.read(audio_path)
            # Convertir en tensor PyTorch et ajouter dimension channel si nÃ©cessaire
            waveform = torch.from_numpy(waveform.T).float()
            if waveform.dim() == 1:
                waveform = waveform.unsqueeze(0)  # Ajouter dimension channel
            
            # Pyannote attend un dict avec 'waveform' et 'sample_rate'
            audio_dict = {
                "waveform": waveform,
                "sample_rate": sample_rate
            }
            
            diarization = self.diarization_pipeline(
                audio_dict,
                num_speakers=None,  # Auto-dÃ©tection
                min_speakers=1,
                max_speakers=self.max_speakers
            )            # Convertir les rÃ©sultats de diarization en dict
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    'start': turn.start,
                    'end': turn.end,
                    'speaker': speaker
                })

            num_speakers = len(set(s['speaker'] for s in speaker_segments))

            print("", file=sys.stderr)
            print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", file=sys.stderr)
            print(f"â•‘  âœ… SUCCÃˆS: {num_speakers} locuteur(s) dÃ©tectÃ©(s)                           â•‘", file=sys.stderr)
            print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£", file=sys.stderr)
            print(f"â•‘  {num_speakers} personnage(s) seront crÃ©Ã©s automatiquement              â•‘", file=sys.stderr)
            print("â•‘  Vous pourrez les renommer ou fusionner aprÃ¨s l'extraction     â•‘", file=sys.stderr)
            print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
            print("", file=sys.stderr)

            # Assigner les speakers aux segments Whisper
            return self._merge_transcription_with_diarization(transcription, speaker_segments)

        except Exception as e:
            error_msg = str(e)
            print(f"âŒ ERREUR Diarization: {error_msg}", file=sys.stderr)

            # Messages d'erreur explicites selon le type d'erreur
            if "token" in error_msg.lower() or "unauthorized" in error_msg.lower() or "403" in error_msg:
                print("", file=sys.stderr)
                print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", file=sys.stderr)
                print("â•‘  âŒ ERREUR: Token HuggingFace manquant ou invalide           â•‘", file=sys.stderr)
                print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£", file=sys.stderr)
                print("â•‘  La diarization (dÃ©tection locuteurs) nÃ©cessite un token HF   â•‘", file=sys.stderr)
                print("â•‘                                                                â•‘", file=sys.stderr)
                print("â•‘  ğŸ“ SOLUTION (2 minutes) :                                    â•‘", file=sys.stderr)
                print("â•‘  1. CrÃ©er compte gratuit: https://huggingface.co              â•‘", file=sys.stderr)
                print("â•‘  2. GÃ©nÃ©rer token: https://huggingface.co/settings/tokens     â•‘", file=sys.stderr)
                print("â•‘  3. Accepter conditions du modÃ¨le:                            â•‘", file=sys.stderr)
                print("â•‘     https://huggingface.co/pyannote/speaker-diarization-3.1   â•‘", file=sys.stderr)
                print("â•‘  4. Ajouter dans .env : HF_TOKEN=hf_xxxxx                     â•‘", file=sys.stderr)
                print("â•‘  5. Relancer l'extraction                                     â•‘", file=sys.stderr)
                print("â•‘                                                                â•‘", file=sys.stderr)
                print("â•‘  âš ï¸  Sans token : UN SEUL locuteur sera crÃ©Ã©                 â•‘", file=sys.stderr)
                print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
                print("", file=sys.stderr)
            elif "model" in error_msg.lower() or "download" in error_msg.lower():
                print("", file=sys.stderr)
                print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", file=sys.stderr)
                print("â•‘  âŒ ERREUR: Ã‰chec du tÃ©lÃ©chargement du modÃ¨le                â•‘", file=sys.stderr)
                print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£", file=sys.stderr)
                print("â•‘  Le modÃ¨le de diarization n'a pas pu Ãªtre tÃ©lÃ©chargÃ©          â•‘", file=sys.stderr)
                print("â•‘                                                                â•‘", file=sys.stderr)
                print("â•‘  ğŸ“ VÃ‰RIFIER :                                                â•‘", file=sys.stderr)
                print("â•‘  - Connexion internet active                                   â•‘", file=sys.stderr)
                print("â•‘  - Espace disque disponible (~1GB)                            â•‘", file=sys.stderr)
                print("â•‘  - Token HF valide et conditions acceptÃ©es                    â•‘", file=sys.stderr)
                print("â•‘                                                                â•‘", file=sys.stderr)
                print("â•‘  âš ï¸  Fallback : UN SEUL locuteur sera crÃ©Ã©                   â•‘", file=sys.stderr)
                print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
                print("", file=sys.stderr)
            else:
                print("", file=sys.stderr)
                print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—", file=sys.stderr)
                print("â•‘  âš ï¸  AVERTISSEMENT: Diarization Ã©chouÃ©e                      â•‘", file=sys.stderr)
                print("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£", file=sys.stderr)
                print(f"â•‘  Erreur: {error_msg[:54]:<54} â•‘", file=sys.stderr)
                print("â•‘                                                                â•‘", file=sys.stderr)
                print("â•‘  âš ï¸  Fallback : UN SEUL locuteur sera crÃ©Ã©                   â•‘", file=sys.stderr)
                print("â•‘  ğŸ’¡ Vous pourrez renommer/fusionner aprÃ¨s l'extraction        â•‘", file=sys.stderr)
                print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•", file=sys.stderr)
                print("", file=sys.stderr)

            print("âš ï¸  Fallback: Attribution de tous les dialogues Ã  SPEAKER_00", file=sys.stderr)
            return self._assign_single_speaker(transcription)

    def _merge_transcription_with_diarization(
        self,
        transcription: Dict,
        speaker_segments: List[Dict]
    ) -> List[Dict]:
        """
        Fusionner la transcription Whisper avec les rÃ©sultats de diarization

        Args:
            transcription: RÃ©sultat Whisper avec segments
            speaker_segments: RÃ©sultats de diarization avec speakers

        Returns:
            Liste de dialogues avec speakers assignÃ©s
        """
        dialogues = []

        for segment in transcription['segments']:
            seg_start = segment['start']
            seg_end = segment['end']
            seg_mid = (seg_start + seg_end) / 2  # Point milieu du segment

            # Trouver le speaker qui parle au milieu du segment
            assigned_speaker = 'SPEAKER_00'  # DÃ©faut
            max_overlap = 0.0

            for spk_seg in speaker_segments:
                # Calculer le chevauchement
                overlap_start = max(seg_start, spk_seg['start'])
                overlap_end = min(seg_end, spk_seg['end'])
                overlap = max(0, overlap_end - overlap_start)

                if overlap > max_overlap:
                    max_overlap = overlap
                    assigned_speaker = spk_seg['speaker']

            dialogues.append({
                'start': seg_start,
                'end': seg_end,
                'text': segment['text'].strip(),
                'speaker': assigned_speaker,
                'confidence': segment.get('confidence', segment.get('no_speech_prob', 0.0)),
                'language': transcription.get('language', 'unknown')
            })

        return dialogues

    def _assign_single_speaker(self, transcription: Dict) -> List[Dict]:
        """Assigner tous les segments Ã  un speaker unique (fallback)"""
        dialogues = []

        for segment in transcription['segments']:
            dialogues.append({
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'].strip(),
                'speaker': 'SPEAKER_00',
                'confidence': segment.get('confidence', segment.get('no_speech_prob', 0.0)),
                'language': transcription.get('language', 'unknown')
            })

        return dialogues

    def process_video(self, video_path: str, output_json: str) -> Dict:
        """
        Pipeline complet d'extraction de dialogues

        Args:
            video_path: Chemin vers la vidÃ©o
            output_json: Chemin de sortie JSON

        Returns:
            DonnÃ©es extraites
        """
        audio_path = None

        try:
            # Ã‰tape 1: Extraction audio (0-20%)
            audio_path = self.extract_audio(video_path)

            # Ã‰tape 2: Transcription Whisper (20-70%)
            transcription = self.transcribe_audio(audio_path)

            # LibÃ©rer mÃ©moire aprÃ¨s transcription
            self._unload_whisper_model()

            # Ã‰tape 3: Diarization (70-90%)
            dialogues = self.apply_diarization(audio_path, transcription)

            # Ã‰tape 4: Formater rÃ©sultat (90-100%)
            speakers = list(set(d['speaker'] for d in dialogues))
            speakers.sort()

            result = {
                'dialogues': dialogues,
                'speakers': speakers,
                'metadata': {
                    'duration': transcription.get('duration', 0),
                    'total_dialogues': len(dialogues),
                    'model': self.model_name,
                    'language': transcription.get('language', 'unknown'),
                    'detected_speakers': len(speakers)
                }
            }

            # Sauvegarder JSON
            with open(output_json, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"âœ… RÃ©sultat sauvegardÃ©: {output_json}")
            print(f"ğŸ“Š Statistiques:")
            print(f"   - Dialogues: {len(dialogues)}")
            print(f"   - Locuteurs: {len(speakers)}")
            print(f"   - DurÃ©e: {result['metadata']['duration']:.1f}s")
            print(f"   - Langue dÃ©tectÃ©e: {result['metadata']['language']}")

            return result

        finally:
            # Nettoyage fichier audio temporaire
            if audio_path and os.path.exists(audio_path):
                os.unlink(audio_path)
                print(f"ğŸ—‘ï¸  Fichier audio temporaire supprimÃ©")

            # LibÃ©rer mÃ©moire
            self._unload_whisper_model()
            gc.collect()


def main():
    parser = argparse.ArgumentParser(
        description="Extraction automatique de dialogues avec Whisper + diarization"
    )
    parser.add_argument('video_path', help="Chemin vers la vidÃ©o")
    parser.add_argument('output_json', help="Chemin de sortie JSON")
    parser.add_argument('--model', default='tiny', choices=list(DialogueExtractor.MODELS.keys()),
                        help="ModÃ¨le Whisper (dÃ©faut: tiny)")
    parser.add_argument('--language', default='auto',
                        help="Langue source (auto/en/fr/zh/ja/..., dÃ©faut: auto)")
    parser.add_argument('--max-speakers', type=int, default=10,
                        help="Nombre max de locuteurs (dÃ©faut: 10)")

    args = parser.parse_args()

    try:
        extractor = DialogueExtractor(
            model_name=args.model,
            language=args.language,
            max_speakers=args.max_speakers
        )

        result = extractor.process_video(args.video_path, args.output_json)

        print("\nâœ… Extraction terminÃ©e avec succÃ¨s!")
        sys.exit(0)

    except Exception as e:
        print(f"\nâŒ Erreur: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
